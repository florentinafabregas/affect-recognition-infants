{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from facenet_pytorch import MTCNN # ?? \n",
    "from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list, EmotiEffLibRecognizerTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = '../data/CityInfant/BlackWhite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/AU/pyafar_infants.csv')\n",
    "df = df.iloc[:,[0,1,-2]]\n",
    "df['imageCat'] = df['imageCat'].map({1: 0, 2: 1, 3: 2})\n",
    "df.to_csv(\"../data/CityInfant/Validationinfo/labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "image_names = df['Image'].tolist()\n",
    "affect_labels = df['imageCat'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "23\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# Split data \n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(image_names, affect_labels, test_size=0.3, stratify=affect_labels, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_dev))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Custom Dataset Class ---\n",
    "\n",
    "class AffectDataset(Dataset):\n",
    "    def __init__(self, image_names, labels, images_folder, transform=None):\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.images_folder, img_name)\n",
    "\n",
    "        try:\n",
    "            image = cv2.imread(image_path) # Load image with OpenCV\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not read image at: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert color (if needed)\n",
    "            image = Image.fromarray(image) #Convert to PIL Image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {image_path} - {e}\")\n",
    "            raise  # Re-raise the exception\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "image_size = 224 # Or 260, depending on the EmotiEffLib model\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle None values in the batch.\n",
    "    \"\"\"\n",
    "    # Filter out None values\n",
    "    batch = [data for data in batch if data is not None]\n",
    "\n",
    "    if not batch:\n",
    "        return None, None  # Return None for both images and labels if batch is empty\n",
    "\n",
    "    images = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # Check if images and labels are empty after filtering\n",
    "    if not images or not labels:\n",
    "        return None, None\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)  # Ensure labels are torch.long\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = AffectDataset(X_train, y_train, images_folder, transform=data_transforms)\n",
    "dev_dataset = AffectDataset(X_dev, y_dev, images_folder, transform=data_transforms)\n",
    "test_dataset = AffectDataset(X_test, y_test, images_folder, transform=data_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last = True)  # Adjust num_workers as needed\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last = True)\n",
    "test_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_model_list()[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EmotiEffLib recognizer\n",
    "recognizer = EmotiEffLibRecognizer(model_name=model_name, engine=\"torch\", device=device)\n",
    "\n",
    "# Access the underlying PyTorch model \n",
    "model = recognizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune -- modify classifier to our classes \n",
    "\n",
    "num_classes = len(np.unique(affect_labels))\n",
    "\n",
    "in_features = recognizer.classifier_weights.shape[1]\n",
    "\n",
    "# Replace the classifier with a new linear layer\n",
    "model.classifier = nn.Linear(in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #Standard cross entropy loss\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0109, Accuracy: 0.4792\n",
      "Epoch 2/10, Loss: 0.8590, Accuracy: 0.6771\n",
      "Epoch 3/10, Loss: 0.7894, Accuracy: 0.8229\n",
      "Epoch 4/10, Loss: 0.7346, Accuracy: 0.8542\n",
      "Epoch 5/10, Loss: 0.6445, Accuracy: 0.8438\n",
      "Epoch 6/10, Loss: 0.6072, Accuracy: 0.8542\n",
      "Epoch 7/10, Loss: 0.5972, Accuracy: 0.8333\n",
      "Epoch 8/10, Loss: 0.5085, Accuracy: 0.8854\n",
      "Epoch 9/10, Loss: 0.4913, Accuracy: 0.8646\n",
      "Epoch 10/10, Loss: 0.4156, Accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(images) #Forward pass\n",
    "        loss = criterion(outputs, labels) #Calculate the loss\n",
    "        loss.backward() #Backpropagation\n",
    "        optimizer.step() #Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x185c6dad0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m         correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, correct\u001b[38;5;241m/\u001b[39mtotal)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# --- 10. Validation ---\n",
    "model.eval() #Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images) #Forward pass\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: ', correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
